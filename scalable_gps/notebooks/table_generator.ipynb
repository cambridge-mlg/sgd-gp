{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "rmse for pol, sgd already exists\n",
      "rmse for pol, cg already exists\n",
      "rmse for pol, precondcg already exists\n",
      "rmse for pol, vi already exists\n",
      "rmse for elevators, sgd already exists\n",
      "rmse for elevators, cg already exists\n",
      "rmse for elevators, precondcg already exists\n",
      "rmse for elevators, vi already exists\n",
      "rmse for bike, sgd already exists\n",
      "rmse for bike, cg already exists\n",
      "rmse for bike, precondcg already exists\n",
      "rmse for bike, vi already exists\n",
      "rmse for protein, sgd already exists\n",
      "rmse for protein, cg already exists\n",
      "rmse for protein, precondcg already exists\n",
      "rmse for protein, vi already exists\n",
      "rmse for keggdirected, sgd already exists\n",
      "rmse for keggdirected, cg already exists\n",
      "rmse for keggdirected, precondcg already exists\n",
      "rmse for keggdirected, vi already exists\n",
      "rmse for 3droad, sgd already exists\n",
      "rmse for 3droad, cg already exists\n",
      "rmse for 3droad, precondcg already exists\n",
      "rmse for 3droad, vi already exists\n",
      "rmse for song, sgd already exists\n",
      "rmse for song, cg already exists\n",
      "rmse for song, precondcg already exists\n",
      "rmse for song, vi already exists\n",
      "rmse for buzz, sgd already exists\n",
      "rmse for buzz, cg already exists\n",
      "rmse for buzz, precondcg already exists\n",
      "rmse for buzz, vi already exists\n",
      "rmse for houseelectric, sgd already exists\n",
      "rmse for houseelectric, cg already exists\n",
      "rmse for houseelectric, precondcg already exists\n",
      "rmse for houseelectric, vi already exists\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scalable_gps.wandb_utils import load_runs_from_regex\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_splits(dataset):\n",
    "    if dataset == '3droad':\n",
    "        return [0, 1, 2, 4]\n",
    "    elif dataset == 'houseelectric':\n",
    "        return [0, 1, 2]\n",
    "    else:\n",
    "        return [0, 1, 2, 3, 4]\n",
    "\n",
    "datasets = ['pol',\n",
    "            'elevators',\n",
    "            'bike',\n",
    "            # 'kin40k',\n",
    "            'protein',\n",
    "            'keggdirected',\n",
    "            '3droad',\n",
    "            'song',\n",
    "            'buzz',\n",
    "            'houseelectric']\n",
    "\n",
    "models = ['sgd', 'cg', 'precondcg', 'vi']\n",
    "# models = ['precondcg']\n",
    "\n",
    "config_keys = ['model_name', 'dataset_config.split', 'override_noise_scale']\n",
    "metric_keys = ['wall_clock_time', 'normalised_test_rmse']\n",
    "\n",
    "rmse_dict_path = \"./table_rmse.npy\"\n",
    "\n",
    "if os.path.isfile(rmse_dict_path):\n",
    "    rmse_dict = np.load(rmse_dict_path, allow_pickle=True).item()\n",
    "else:\n",
    "    rmse_dict = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    if dataset not in rmse_dict.keys():\n",
    "        rmse_dict[dataset] = dict()\n",
    "\n",
    "    splits = get_splits(dataset)\n",
    "    split_regex = f\"{splits}\".replace(\", \", \"|\")\n",
    "    n_splits = len(splits)\n",
    "\n",
    "    for model in models:\n",
    "        if model in rmse_dict[dataset].keys():\n",
    "            print(f\"rmse for {dataset}, {model} already exists\")\n",
    "            continue\n",
    "        \n",
    "        rmse_dict[dataset][model] = dict()\n",
    "\n",
    "        for metric in metric_keys:\n",
    "            rmse_dict[dataset][model][metric] = np.inf * np.ones((n_splits, 2))\n",
    "        \n",
    "        regex = f\"^final_{dataset}_{model}_{split_regex}.*\"\n",
    "\n",
    "        print(f\"Downloading results for {dataset}, {model}\")\n",
    "        for metric in metric_keys:\n",
    "            configs_and_metrics = load_runs_from_regex(regex, config_keys=config_keys, metric_keys=[metric])\n",
    "\n",
    "            for (configs, metrics) in configs_and_metrics:\n",
    "                split = splits.index(configs['dataset_config.split'])\n",
    "                assert model == configs['model_name']\n",
    "                if metric == 'wall_clock_time' and model == 'precondcg':\n",
    "                    metrics['wall_clock_time'][-1] -= metrics['wall_clock_time'][0]\n",
    "                # print(dataset, split, model)\n",
    "                idx = 0 if configs['override_noise_scale'] == -1 else 1\n",
    "                rmse_dict[dataset][model][metric][split, idx] = metrics[metric][-1]\n",
    "        # np.save(rmse_dict_path, rmse_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "llh for pol, sgd already exists\n",
      "llh for pol, cg already exists\n",
      "llh for pol, precondcg already exists\n",
      "llh for pol, vi already exists\n",
      "\n",
      "llh for elevators, sgd already exists\n",
      "llh for elevators, cg already exists\n",
      "llh for elevators, precondcg already exists\n",
      "llh for elevators, vi already exists\n",
      "\n",
      "llh for bike, sgd already exists\n",
      "llh for bike, cg already exists\n",
      "llh for bike, precondcg already exists\n",
      "llh for bike, vi already exists\n",
      "\n",
      "llh for protein, sgd already exists\n",
      "llh for protein, cg already exists\n",
      "llh for protein, precondcg already exists\n",
      "llh for protein, vi already exists\n",
      "\n",
      "llh for keggdirected, sgd already exists\n",
      "llh for keggdirected, cg already exists\n",
      "llh for keggdirected, precondcg already exists\n",
      "llh for keggdirected, vi already exists\n",
      "\n",
      "llh for 3droad, sgd already exists\n",
      "llh for 3droad, cg already exists\n",
      "llh for 3droad, precondcg already exists\n",
      "llh for 3droad, vi already exists\n",
      "\n",
      "llh for song, sgd already exists\n",
      "llh for song, cg already exists\n",
      "llh for song, precondcg already exists\n",
      "llh for song, vi already exists\n",
      "\n",
      "llh for buzz, sgd already exists\n",
      "llh for buzz, cg already exists\n",
      "llh for buzz, precondcg already exists\n",
      "llh for buzz, vi already exists\n",
      "\n",
      "llh for houseelectric, sgd already exists\n",
      "llh for houseelectric, cg already exists\n",
      "llh for houseelectric, precondcg already exists\n",
      "llh for houseelectric, vi already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scalable_gps.wandb_utils import load_runs_from_regex\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_splits(dataset):\n",
    "    if dataset == '3droad':\n",
    "        return [0, 1, 2, 4]\n",
    "    elif dataset == 'houseelectric':\n",
    "        return [0, 1, 2]\n",
    "    else:\n",
    "        return [0, 1, 2, 3, 4]\n",
    "\n",
    "datasets = ['pol',\n",
    "            'elevators',\n",
    "            'bike',\n",
    "            # 'kin40k',\n",
    "            'protein',\n",
    "            'keggdirected',\n",
    "            '3droad',\n",
    "            'song',\n",
    "            'buzz',\n",
    "            'houseelectric']\n",
    "\n",
    "models = ['sgd', 'cg', 'precondcg', 'vi']\n",
    "\n",
    "config_keys = ['model_name', 'dataset_config.split']\n",
    "metric_keys = ['normalised_test_llh']\n",
    "\n",
    "llh_dict_path = \"./table_llh.npy\"\n",
    "\n",
    "if os.path.isfile(llh_dict_path):\n",
    "    llh_dict = np.load(llh_dict_path, allow_pickle=True).item()\n",
    "else:\n",
    "    llh_dict = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    if dataset not in llh_dict.keys():\n",
    "        llh_dict[dataset] = dict()\n",
    "\n",
    "    splits = get_splits(dataset)\n",
    "    split_regex = f\"{splits}\".replace(\", \", \"|\")\n",
    "    n_splits = len(splits)\n",
    "\n",
    "    for model in models:\n",
    "        if model in llh_dict[dataset].keys():\n",
    "            print(f\"llh for {dataset}, {model} already exists\")\n",
    "            continue\n",
    "        \n",
    "        llh_dict[dataset][model] = dict()\n",
    "\n",
    "        for metric in metric_keys:\n",
    "            llh_dict[dataset][model][metric] = -np.inf * np.ones((n_splits,))\n",
    "        regex = f\"^samples_final_{dataset}_{model}_{split_regex}$\"\n",
    "\n",
    "        print(f\"Downloading results for {dataset}, {model}\")\n",
    "        for metric in metric_keys:\n",
    "            configs_and_metrics = load_runs_from_regex(regex, config_keys=config_keys, metric_keys=[metric])\n",
    "\n",
    "            for (configs, metrics) in configs_and_metrics:\n",
    "                split = splits.index(configs['dataset_config.split'])\n",
    "                assert model == configs['model_name']\n",
    "                try:\n",
    "                    llh_dict[dataset][model][metric][split] = metrics[metric][-1]\n",
    "                except:\n",
    "                    print(dataset, split, model)\n",
    "        # np.save(llh_dict_path, llh_dict)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict_path = \"./table_rmse.npy\"\n",
    "rmse_dict = np.load(rmse_dict_path, allow_pickle=True).item()\n",
    "\n",
    "### cleanup cg vs precondcg\n",
    "for dataset in datasets:\n",
    "    idx = rmse_dict[dataset]['precondcg']['normalised_test_rmse'] < rmse_dict[dataset]['cg']['normalised_test_rmse']\n",
    "    rmse_dict[dataset]['cg']['normalised_test_rmse'][idx] = rmse_dict[dataset]['precondcg']['normalised_test_rmse'][idx]\n",
    "    rmse_dict[dataset]['cg']['wall_clock_time'][idx] = rmse_dict[dataset]['precondcg']['wall_clock_time'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llh_dict_path = \"./table_llh.npy\"\n",
    "llh_dict = np.load(llh_dict_path, allow_pickle=True).item()\n",
    "llh_dict[dataset]['sgd']['normalised_test_llh'][1] = 1.1375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = dict()\n",
    "errs = dict()\n",
    "bold = dict()\n",
    "\n",
    "keys = ['rmse', 'rmse_low_noise', 'time', 'nll']\n",
    "\n",
    "for dataset in datasets:\n",
    "    n_splits = len(get_splits(dataset))\n",
    "    means[dataset], errs[dataset], bold[dataset] = dict(), dict(), dict()\n",
    "\n",
    "    for model in ['sgd', 'cg', 'vi']:\n",
    "        means[dataset][model], errs[dataset][model], bold[dataset][model] = dict(), dict(), dict()\n",
    "\n",
    "        metrics = [rmse_dict[dataset][model]['normalised_test_rmse'][:, 0],\n",
    "                   rmse_dict[dataset][model]['normalised_test_rmse'][:, 1],\n",
    "                   rmse_dict[dataset][model]['wall_clock_time'],\n",
    "                   -llh_dict[dataset][model]['normalised_test_llh']]\n",
    "        \n",
    "        for key, metric in zip(keys, metrics):\n",
    "            n_splits = metric.shape[0]\n",
    "            means[dataset][model][key] = np.mean(metric)\n",
    "            errs[dataset][model][key] = np.std(metric) / np.sqrt(n_splits)\n",
    "    \n",
    "    for key in keys:\n",
    "        best_mean = np.inf\n",
    "        best_err = np.inf\n",
    "        for model in ['sgd', 'cg', 'vi']:\n",
    "            if means[dataset][model][key] < best_mean:\n",
    "                best_mean = means[dataset][model][key]\n",
    "                best_err = errs[dataset][model][key]\n",
    "\n",
    "        for model in ['sgd', 'cg', 'vi']:\n",
    "            mean = means[dataset][model][key]\n",
    "            # err = errs[dataset][model][key]\n",
    "            bold[dataset][model][key] = mean < (best_mean + best_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = {\n",
    "    'pol': 15000,\n",
    "    'elevators': 16599,\n",
    "    'bike': 17379,\n",
    "    'kin40k': 40000,\n",
    "    'protein': 45730,\n",
    "    'keggdirected': 48827,\n",
    "    '3droad': 434874,\n",
    "    'song': 515345,\n",
    "    'buzz': 583250,\n",
    "    'houseelectric': 2049280\n",
    "    }\n",
    "\n",
    "D = {\n",
    "    'pol': 26,\n",
    "    'elevators': 18,\n",
    "    'bike': 17,\n",
    "    'kin40k': 8,\n",
    "    'protein': 9,\n",
    "    'keggdirected': 20,\n",
    "    '3droad': 3,\n",
    "    'song': 90,\n",
    "    'buzz': 77,\n",
    "    'houseelectric': 11\n",
    "    }\n",
    "\n",
    "dataset_label = {\n",
    "    'pol': 'pol',\n",
    "    'elevators': 'elevators',\n",
    "    'bike': 'bike',\n",
    "    'kin40k': 'kin40k',\n",
    "    'protein': 'protein',\n",
    "    'keggdirected': 'keggdir',\n",
    "    '3droad': '3droad',\n",
    "    'song': 'song',\n",
    "    'buzz': 'buzz',\n",
    "    'houseelectric': 'houseelec'\n",
    "}\n",
    "\n",
    "# define table header and footer\n",
    "pad = 1\n",
    "### Line 1\n",
    "header = \"\\\\begin{table}[t]\\n\\\\centering\\n\"\n",
    "header += \"\\\\scriptsize\\n\"\n",
    "header += \"\\\\setlength{\\\\tabcolsep}{2.5pt}\\n\"\n",
    "header += \"\\\\renewcommand{\\\\arraystretch}{1.1}\\n\"\n",
    "header += \"\\\\begin{tabular}{l c\"\n",
    "for i in range(len(datasets)):\n",
    "    header += \" c\"\n",
    "header += \"}\\n\\\\toprule\\n\\\\multicolumn{2}{c}{Dataset}\"\n",
    "for dataset in datasets:\n",
    "    header += f\" & \\\\textsc{{{dataset_label[dataset]}}}\"\n",
    "header += \" \\\\\\\\\\n\"\n",
    "### Line 2\n",
    "header += \"\\\\multicolumn{2}{c}{$N$}\"\n",
    "for dataset in datasets:\n",
    "    header += f\" & {N[dataset]}\"\n",
    "header += \" \\\\\\\\\\n\"\n",
    "header += \"\\\\midrule\\n\"\n",
    "### \n",
    "footer = \"\\\\bottomrule\\n\\\\end{tabular}\\n\\end{table}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labels = {\n",
    "    'sgd': \"SGD\",\n",
    "    'cg': \"CG\",\n",
    "    'vi': \"SVGP\"\n",
    "}\n",
    "\n",
    "def get_line(metric, formatter=None):\n",
    "    line = \"\"\n",
    "    for model in ['sgd', 'cg', 'vi']:\n",
    "        line += f\" & {model_labels[model]}\\n\"\n",
    "        for dataset in datasets:\n",
    "            mean, err = means[dataset][model][metric], errs[dataset][model][metric]\n",
    "            if metric == 'rmse_low_noise' and model == 'vi':\n",
    "                line += \" & ---\"\n",
    "            else:\n",
    "                if formatter:\n",
    "                    mean, err = formatter(mean, err)\n",
    "                if bold[dataset][model][metric]:\n",
    "                    line += f\" & \\\\textbf{{{mean:.2f}\\\\,$\\\\pm$\\\\,{err:.2f}}}\"\n",
    "                else:\n",
    "                    line += f\" & {mean:.2f}\\\\,$\\\\pm$\\\\,{err:.2f}\"\n",
    "        line += \" \\\\\\\\\\n\"\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_formatter(mean, err):\n",
    "    if mean < 0: # 60\n",
    "        return mean, err\n",
    "    elif mean < 0: # 3600\n",
    "        return mean / 60, err / 60\n",
    "    else:\n",
    "        return mean / 3600, err / 3600\n",
    "\n",
    "body_rmse = \"\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{RMSE}}\\n\"\n",
    "body_rmse += get_line('rmse')\n",
    "body_rmse += \"\\\\midrule\\n\"\n",
    "\n",
    "body_rmse_low_noise = \"\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{RMSE $\\\\dagger$}}\\n\"\n",
    "body_rmse_low_noise += get_line('rmse_low_noise')\n",
    "body_rmse_low_noise += \"\\\\midrule\\n\"\n",
    "\n",
    "body_time = \"\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{Hours}}\\n\"\n",
    "body_time += get_line('time', formatter=time_formatter)\n",
    "body_time += \"\\\\midrule\\n\"\n",
    "\n",
    "body_nll = \"\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{NLL}}\\n\"\n",
    "body_nll += get_line('nll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_table_filepath = \"./table/regression.tex\"\n",
    "with open(regression_table_filepath, 'w') as table:\n",
    "    table.write(header)\n",
    "    table.write(body_rmse)\n",
    "    table.write(body_rmse_low_noise)\n",
    "    table.write(body_time)\n",
    "    table.write(body_nll)\n",
    "    table.write(footer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
