{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_splits(dataset):\n",
    "    if dataset == '3droad':\n",
    "        return [0, 1, 2, 4]\n",
    "    elif dataset == 'houseelectric':\n",
    "        return [0, 1, 2]\n",
    "    else:\n",
    "        return [0, 1, 2, 3, 4]\n",
    "\n",
    "clustering_ratios = [0.5, 1, 2]\n",
    "\n",
    "datasets = ['3droad', 'song', 'buzz', 'houseelectric']\n",
    "\n",
    "n_steps = 100\n",
    "\n",
    "results_path = \"./inducing_sgd_rmse.npy\"\n",
    "results_rmse = np.load(results_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_llh_house = np.load(\"./inducing_sgd_llh.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:33<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# load song samples\n",
    "from scalable_gps.wandb_utils import load_runs_from_sweep\n",
    "\n",
    "sweep_id = 't7cht6nj'\n",
    "config_keys_llh = ['dataset_name', 'dataset_config.split', 'inducing_train_config.clustering_length_scale_ratio']\n",
    "metric_keys_llh = ['kept_points', 'normalised_test_llh']\n",
    "configs_and_metrics_llh_song = load_runs_from_sweep(sweep_id, config_keys_llh, metric_keys_llh, entity='javierantoran', project='inducing_sgd_tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song 4 2 -1.202202320098877 463806\n",
      "song 4 1 -1.2021652460098267 463804\n",
      "song 4 0.5 -1.2021652460098267 463804\n",
      "song 3 2 -1.1998318433761597 463806\n",
      "song 3 0.5 -1.1999462842941284 463804\n",
      "song 3 1 -1.1998318433761597 463806\n",
      "song 2 2 -1.2089701890945435 463804\n",
      "song 2 1 -1.2090787887573242 463802\n",
      "song 2 0.5 -1.2090787887573242 463802\n",
      "song 1 2 -1.2158271074295044 463804\n",
      "song 1 1 -1.2157727479934692 463802\n",
      "song 1 0.5 -1.2157599925994873 463802\n",
      "song 0 2 -1.1990042924880981 463807\n",
      "song 0 1 -1.1990480422973633 463805\n",
      "song 0 0.5 -1.1989991664886475 463803\n"
     ]
    }
   ],
   "source": [
    "# load samples / LLH\n",
    "dataset = 'song'\n",
    "results_path = \"./inducing_sgd_llh_song.npy\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results_llh_song = np.load(results_path, allow_pickle=True).item()\n",
    "else:\n",
    "    results_llh_song = dict()\n",
    "\n",
    "splits = get_splits(dataset)\n",
    "n_splits = len(splits)\n",
    "results_llh_song['normalised_test_llh'] = np.zeros((n_splits, len(clustering_ratios)))\n",
    "results_llh_song['kept_points'] = np.zeros((n_splits, len(clustering_ratios)))\n",
    "\n",
    "for (configs, metrics) in configs_and_metrics_llh_song:\n",
    "    # dataset = configs['dataset_name']\n",
    "    split = configs['dataset_config.split']\n",
    "    ratio = configs['inducing_train_config.clustering_length_scale_ratio']\n",
    "\n",
    "    splits = get_splits(dataset)\n",
    "    if split not in splits:\n",
    "        continue\n",
    "    if ratio not in clustering_ratios:\n",
    "        continue\n",
    "    print(dataset, split, ratio, metrics['normalised_test_llh'][-1], metrics['kept_points'][0])\n",
    "\n",
    "    split_idx = splits.index(split)\n",
    "    clustering_idx = clustering_ratios.index(ratio)\n",
    "\n",
    "    results_llh_song['normalised_test_llh'][split_idx, clustering_idx] = metrics['normalised_test_llh'][-1]\n",
    "    results_llh_song['kept_points'][split_idx, clustering_idx] = metrics['kept_points'][0]\n",
    "np.save(results_path, results_llh_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:47<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# load song samples\n",
    "from scalable_gps.wandb_utils import load_runs_from_sweep\n",
    "\n",
    "sweep_id = 'camr4x76'\n",
    "config_keys_llh = ['dataset_name', 'dataset_config.split', 'inducing_train_config.clustering_length_scale_ratio']\n",
    "metric_keys_llh = ['kept_points']\n",
    "configs_and_metrics_llh_song = load_runs_from_sweep(sweep_id, config_keys_llh, metric_keys_llh, entity='javierantoran', project='inducing_sgd_tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buzz 4 2 467190\n",
      "buzz 4 0.5 353086\n",
      "buzz 4 1 412081\n",
      "buzz 3 2 500061\n",
      "buzz 3 1 477409\n",
      "buzz 3 0.5 426472\n",
      "buzz 2 2 479169\n",
      "buzz 2 1 427562\n",
      "buzz 2 0.5 368630\n",
      "buzz 1 2 480944\n",
      "buzz 1 1 430822\n",
      "buzz 1 0.5 372103\n",
      "buzz 0 1 452794\n",
      "buzz 0 2 492736\n",
      "buzz 0 0.5 396025\n"
     ]
    }
   ],
   "source": [
    "# load samples / LLH\n",
    "results_path = \"./inducing_sgd_llh_buzz.npy\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results_llh_buzz = np.load(results_path, allow_pickle=True).item()\n",
    "else:\n",
    "    results_llh_buzz = dict()\n",
    "\n",
    "splits = get_splits(dataset)\n",
    "n_splits = len(splits)\n",
    "results_llh_buzz['kept_points'] = np.zeros((n_splits, len(clustering_ratios)))\n",
    "\n",
    "for (configs, metrics) in configs_and_metrics_llh_song:\n",
    "    dataset = configs['dataset_name']\n",
    "    if dataset != 'buzz':\n",
    "        continue\n",
    "    split = configs['dataset_config.split']\n",
    "    ratio = configs['inducing_train_config.clustering_length_scale_ratio']\n",
    "\n",
    "    splits = get_splits(dataset)\n",
    "    if split not in splits:\n",
    "        continue\n",
    "    if ratio not in clustering_ratios:\n",
    "        continue\n",
    "    print(dataset, split, ratio, metrics['kept_points'][0])\n",
    "\n",
    "    split_idx = splits.index(split)\n",
    "    clustering_idx = clustering_ratios.index(ratio)\n",
    "\n",
    "    results_llh_buzz['kept_points'][split_idx, clustering_idx] = metrics['kept_points'][0]\n",
    "np.save(results_path, results_llh_buzz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:50<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# load song samples\n",
    "from scalable_gps.wandb_utils import load_runs_from_sweep\n",
    "\n",
    "sweep_id = 'camr4x76'\n",
    "config_keys_llh = ['dataset_name', 'dataset_config.split', 'inducing_train_config.clustering_length_scale_ratio']\n",
    "metric_keys_llh = ['normalised_test_llh']\n",
    "configs_and_metrics_llh_song = load_runs_from_sweep(sweep_id, config_keys_llh, metric_keys_llh, entity='javierantoran', project='inducing_sgd_tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buzz 4 2 -0.3711528182029724\n",
      "buzz 4 0.5 -0.3629302978515625\n",
      "buzz 4 1 -0.3713728189468384\n",
      "buzz 3 2 -0.4652887284755707\n",
      "buzz 3 1 -0.46351245045661926\n",
      "buzz 3 0.5 -0.4632129371166229\n",
      "buzz 2 2 -0.873681366443634\n",
      "buzz 2 1 -0.884199321269989\n",
      "buzz 2 0.5 -0.8861228823661804\n",
      "buzz 1 2 -0.4217495024204254\n",
      "buzz 1 1 -0.4207931160926819\n",
      "buzz 1 0.5 -0.41431036591529846\n",
      "buzz 0 1 -0.5299805402755737\n",
      "buzz 0 2 -0.5331936478614807\n",
      "buzz 0 0.5 -0.5247743129730225\n"
     ]
    }
   ],
   "source": [
    "# load samples / LLH\n",
    "results_path = \"./inducing_sgd_llh_buzz.npy\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results_llh_buzz = np.load(results_path, allow_pickle=True).item()\n",
    "else:\n",
    "    results_llh_buzz = dict()\n",
    "\n",
    "splits = get_splits(dataset)\n",
    "n_splits = len(splits)\n",
    "results_llh_buzz['normalised_test_llh'] = np.zeros((n_splits, len(clustering_ratios)))\n",
    "\n",
    "for (configs, metrics) in configs_and_metrics_llh_song:\n",
    "    dataset = configs['dataset_name']\n",
    "    if dataset != 'buzz':\n",
    "        continue\n",
    "    split = configs['dataset_config.split']\n",
    "    ratio = configs['inducing_train_config.clustering_length_scale_ratio']\n",
    "\n",
    "    splits = get_splits(dataset)\n",
    "    if split not in splits:\n",
    "        continue\n",
    "    if ratio not in clustering_ratios:\n",
    "        continue\n",
    "    if not metrics['normalised_test_llh']:\n",
    "        continue\n",
    "    print(dataset, split, ratio, metrics['normalised_test_llh'][-1])\n",
    "\n",
    "    split_idx = splits.index(split)\n",
    "    clustering_idx = clustering_ratios.index(ratio)\n",
    "\n",
    "    results_llh_buzz['normalised_test_llh'][split_idx, clustering_idx] = metrics['normalised_test_llh'][-1]\n",
    "results_llh_buzz[3, 0] = -0.4632\n",
    "results_llh_buzz[3, 1] = -0.4635\n",
    "results_llh_buzz[3, 2] = -0.4653\n",
    "np.save(results_path, results_llh_buzz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [01:27<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# load song samples\n",
    "from scalable_gps.wandb_utils import load_runs_from_sweep\n",
    "\n",
    "sweep_id = 'camr4x76'\n",
    "config_keys_llh = ['dataset_name', 'dataset_config.split', 'inducing_train_config.clustering_length_scale_ratio']\n",
    "metric_keys_llh = ['kept_points', 'normalised_test_llh']\n",
    "configs_and_metrics_llh_song = load_runs_from_sweep(sweep_id, config_keys_llh, metric_keys_llh, entity='javierantoran', project='inducing_sgd_tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3droad 4 2 0.6190418004989624 389992\n",
      "3droad 4 0.5 0.6062174439430237 355655\n",
      "3droad 4 1 0.6166220307350159 382816\n",
      "3droad 2 2 0.618267297744751 388111\n",
      "3droad 2 1 0.6144659519195557 376144\n",
      "3droad 2 0.5 0.6023598909378052 342493\n",
      "3droad 1 2 0.6309990286827087 387123\n",
      "3droad 1 1 0.6262817978858948 372080\n",
      "3droad 1 0.5 0.6105291247367859 330195\n",
      "3droad 0 0.5 0.6068035960197449 348889\n",
      "3droad 0 1 0.619066596031189 379730\n",
      "3droad 0 2 0.621998131275177 388585\n"
     ]
    }
   ],
   "source": [
    "# load samples / LLH\n",
    "results_path = \"./inducing_sgd_llh_3droad.npy\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results_llh_3droad = np.load(results_path, allow_pickle=True).item()\n",
    "else:\n",
    "    results_llh_3droad = dict()\n",
    "\n",
    "splits = get_splits(dataset)\n",
    "n_splits = len(splits)\n",
    "results_llh_3droad['normalised_test_llh'] = np.zeros((n_splits, len(clustering_ratios)))\n",
    "results_llh_3droad['kept_points'] = np.zeros((n_splits, len(clustering_ratios)))\n",
    "\n",
    "for (configs, metrics) in configs_and_metrics_llh_song:\n",
    "    dataset = configs['dataset_name']\n",
    "    if dataset != '3droad':\n",
    "        continue\n",
    "    split = configs['dataset_config.split']\n",
    "    ratio = configs['inducing_train_config.clustering_length_scale_ratio']\n",
    "\n",
    "    splits = get_splits(dataset)\n",
    "    if split not in splits:\n",
    "        continue\n",
    "    if ratio not in clustering_ratios:\n",
    "        continue\n",
    "    print(dataset, split, ratio, metrics['normalised_test_llh'][-1], metrics['kept_points'][0])\n",
    "\n",
    "    split_idx = splits.index(split)\n",
    "    clustering_idx = clustering_ratios.index(ratio)\n",
    "\n",
    "    results_llh_3droad['normalised_test_llh'][split_idx, clustering_idx] = metrics['normalised_test_llh'][-1]\n",
    "    results_llh_3droad['kept_points'][split_idx, clustering_idx] = metrics['kept_points'][0]\n",
    "np.save(results_path, results_llh_3droad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "llh_dict = dict()\n",
    "datasets = ['3droad', 'song', 'buzz', 'houseelectric']\n",
    "for dataset in datasets:\n",
    "    llh_dict[dataset] = dict()\n",
    "llh_dict['3droad']['kept_points'] = results_llh_3droad['kept_points']\n",
    "llh_dict['3droad']['llh'] = results_llh_3droad['normalised_test_llh']\n",
    "llh_dict['song']['kept_points'] = results_llh_song['kept_points']\n",
    "llh_dict['song']['llh'] = results_llh_song['normalised_test_llh']\n",
    "llh_dict['buzz']['kept_points'] = results_llh_buzz['kept_points']\n",
    "llh_dict['buzz']['llh'] = results_llh_buzz['normalised_test_llh']\n",
    "\n",
    "llh_dict['houseelectric']['kept_points'] = results_llh_house['houseelectric']['kept_points']\n",
    "llh_dict['houseelectric']['llh'] = results_llh_house['houseelectric']['normalised_test_llh']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"inducing_sgd_llh_dict.npy\", llh_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dict = np.load(\"inducing_sgd_rmse.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.68858686 24.57540754 49.5827858 ]\n"
     ]
    }
   ],
   "source": [
    "time = np.mean(rmse_dict['houseelectric']['wall_clock_time'][:, :, -1], axis=0)\n",
    "print(time/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['wall_clock_time', 'train/normalised_test_rmse', 'train/test_rmse'])\n"
     ]
    }
   ],
   "source": [
    "datasets = ['3droad', 'song', 'buzz', 'houseelectric']\n",
    "means_time = dict()\n",
    "for dataset in datasets:\n",
    "    means_time[dataset] = np.mean(rmse_dict[dataset]['wall_clock_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = dict()\n",
    "errs = dict()\n",
    "bold = dict()\n",
    "\n",
    "keys = ['time', 'rmse', 'nll', 'kept_points']\n",
    "datasets = ['3droad', 'song', 'buzz', 'houseelectric']\n",
    "\n",
    "for dataset in datasets:\n",
    "    n_splits = len(get_splits(dataset))\n",
    "    means[dataset], errs[dataset], bold[dataset] = dict(), dict(), dict()\n",
    "    for i, ratio in enumerate(clustering_ratios):\n",
    "        means[dataset][ratio], errs[dataset][ratio], bold[dataset][ratio] = dict(), dict(), dict()\n",
    "\n",
    "        metrics = [rmse_dict[dataset]['wall_clock_time'][:, i, -1],\n",
    "                   rmse_dict[dataset]['normalised_test_rmse'][:, 0],\n",
    "                    rmse_dict[dataset][ratio]['normalised_test_rmse'][:, 1],\n",
    "                    -llh_dict[dataset][ratio]['normalised_test_llh']]\n",
    "        \n",
    "        for key, metric in zip(keys, metrics):\n",
    "            n_splits = metric.shape[0]\n",
    "            means[dataset][ratio][key] = np.mean(metric)\n",
    "            errs[dataset][ratio][key] = np.std(metric) / np.sqrt(n_splits)\n",
    "    \n",
    "    for key in keys:\n",
    "        best_mean = np.inf\n",
    "        best_err = np.inf\n",
    "        for model in ['sgd', 'cg', 'vi']:\n",
    "            if means[dataset][model][key] < best_mean:\n",
    "                best_mean = means[dataset][model][key]\n",
    "                best_err = errs[dataset][model][key]\n",
    "\n",
    "        for model in ['sgd', 'cg', 'vi']:\n",
    "            mean = means[dataset][model][key]\n",
    "            # err = errs[dataset][model][key]\n",
    "            bold[dataset][model][key] = mean < (best_mean + best_err)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
